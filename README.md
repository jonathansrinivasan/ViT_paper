# **How Do Vision Transformers Work?**
## **By Namuk Park, Songkuk Kim**

## Problem

### Topics Covered
1. MSAs improves accuracy and generlization by flattening loss landscapes. ViTs suffer from non-convex losses, which can be fixed with large datasets and loss landscape smoothing methods.
2. MSAs and Conv have opposite behaviors, therefore, they are complementary.
3. Multi-stage neural networs behave like a series of small individual models. 
4. Authors propose ALterNet, a model that replaces Conv blocks at the end of a stage with MSA blocks. AlterNet outperforms CNN models in both large and small datasets.   

## Approach

## Solution
